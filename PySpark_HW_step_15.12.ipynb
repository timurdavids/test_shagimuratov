{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S6qBAyuD-lW",
        "outputId": "e1a1b165-7768-4790-aeb4-0238c5ca0211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pip 24.1.2 from /usr/local/lib/python3.12/dist-packages/pip (python 3.12)\n"
          ]
        }
      ],
      "source": [
        "pip --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bc7l4gHNEMwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeElfR3RENYr",
        "outputId": "81a6f265-3c02-4ffd-ea1d-c026ce39a833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "\n",
        "\n",
        "# Настройка Spark\n",
        "\n",
        "conf = SparkConf().setAppName(\"Simple RDD Example\").setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "\n",
        "\n",
        "# 1. Создание RDD из списка чисел\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "\n",
        "\n",
        "# 2. Трансформации: Фильтрация чётных чисел\n",
        "\n",
        "even_numbers_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Действие: Подсчёт суммы чётных чисел\n",
        "\n",
        "sum_even_numbers = even_numbers_rdd.sum()\n",
        "\n",
        "\n",
        "\n",
        "# Вывод результата\n",
        "\n",
        "print(\"Чётные числа:\", even_numbers_rdd.collect())\n",
        "\n",
        "print(\"Сумма чётных чисел:\", sum_even_numbers)\n",
        "\n",
        "\n",
        "\n",
        "a = 5 + 6\n",
        "\n",
        "print(a)\n",
        "\n",
        "# Остановка SparkContext\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PmszkitEX_t",
        "outputId": "4a348665-483b-4588-ed23-f2da427748f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Чётные числа: [2, 4, 6, 8, 10]\n",
            "Сумма чётных чисел: 30\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Создание объекта SparkContext\n",
        "sc = SparkContext(appName=\"MySparkApp\")\n",
        "\n",
        "# Здесь можно выполнять операции с RDD\n",
        "\n",
        "# Закрытие SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "BThwf4i1hNkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание объекта SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Здесь можно выполнять операции с DataFrames\n",
        "# Закрытие SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "yQ-87BeWhTSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание Spark Session с различными параметрами конфигурации\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MyApp\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "    .config(\"spark.checkpoint.dir\", \"/path/to/checkpoint/dir\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/path/to/warehouse/dir\") \\\n",
        "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Пример использования Spark Session\n",
        "df = spark.read.json(\"/path/to/json/file\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "wXpRtqixhplj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
        "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\n",
        "\n",
        "# Явное определение схемы\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Value\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Создание DataFrame с явной схемой\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.printSchema()\n",
        "\n",
        "# Автоматическое определение схемы при чтении данных из CSV\n",
        "df_auto = spark.read.csv(\"/path/to/csv/file\", header=True, inferSchema=True)\n",
        "df_auto.printSchema()"
      ],
      "metadata": {
        "id": "nhCD2pvRhqoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####  Итоговое задание по PySpark"
      ],
      "metadata": {
        "id": "D8oxruqdmGrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "id": "XledgZYQmRyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc321c75-5d33-439e-a0e7-f922c0578251"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVm--xtjigCd",
        "outputId": "c16fa2de-9cfa-4894-cfda-6fbea72da8d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-39.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.3)\n",
            "Downloading faker-39.0.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-39.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "num_records = 100000\n",
        "\n",
        "response_codes = [200, 301, 404, 500]\n",
        "http_methods = ['GET', 'POST', 'PUT', 'DELETE']\n",
        "\n",
        "file_path = \"web_server_logs.csv\"\n",
        "\n",
        "with open(file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ip', 'timestamp', 'method', 'url', 'response_code', 'response_size'])\n",
        "\n",
        "    for i in range(num_records):\n",
        "        ip = fake.ipv4()\n",
        "        timestamp = fake.date_time_this_year().isoformat()\n",
        "        method = random.choice(http_methods)\n",
        "        url = fake.uri_path()\n",
        "        response_code = random.choice(response_codes)\n",
        "        response_size = random.randint(100, 10000)\n",
        "\n",
        "        writer.writerow([ip, timestamp, method, url, response_code, response_size])\n",
        "\n",
        "print(f\"Сгенерировано {num_records} записей и сохранено в {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_AyVLWIilJf",
        "outputId": "2634c457-04b7-48a7-b0f8-a3efb4d023c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сгенерировано 100000 записей и сохранено в web_server_logs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName('ReadCSV').getOrCreate()\n",
        "\n",
        "df = spark.read.csv('/content/web_server_logs.csv', header=True, inferSchema=True)\n",
        "\n",
        "grouped_df_ip = df.groupBy('ip').agg({\"ip\": \"count\"}).withColumnRenamed(\"count(ip)\", \"request_count\")\n",
        "\n",
        "print(f'Top 10 active IP adresses:')\n",
        "grouped_df_ip.show(10)\n",
        "\n",
        "print(f'Request count by HTTP method:')\n",
        "grouped_df_method = df.groupBy('method').agg({'method':'count'}).withColumnRenamed('count(method)', 'method_count')\n",
        "\n",
        "grouped_df_method.show()\n",
        "\n",
        "filtered_df_bad = df.filter(col(\"response_code\") == 404)\n",
        "count_filtererd = filtered_df_bad.agg({'response_code':'count'}).withColumnRenamed('count(response_code)', 'Count of 404 responce codes:')\n",
        "count_filtererd.show()\n",
        "\n",
        "group_date = df.groupBy('timestamp').agg({'response_size':'sum'}).withColumnRenamed('sum(response_size)', 'total_responce_size').withColumnRenamed('timestamp', 'date')\n",
        "ordered = group_date.orderBy(col('date'))\n",
        "print(f'Total responce size by day:')\n",
        "ordered.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUorHmz8i5oJ",
        "outputId": "66f0172f-311c-49f9-8613-5665fca4d80b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 active IP adresses:\n",
            "+---------------+-------------+\n",
            "|             ip|request_count|\n",
            "+---------------+-------------+\n",
            "| 189.144.214.66|            1|\n",
            "|   173.177.90.5|            1|\n",
            "| 46.209.154.122|            1|\n",
            "|   12.48.98.147|            1|\n",
            "|  218.245.85.94|            1|\n",
            "|  16.225.39.147|            1|\n",
            "| 189.75.194.222|            1|\n",
            "|101.241.127.182|            1|\n",
            "|   222.27.68.45|            1|\n",
            "| 145.10.146.148|            1|\n",
            "+---------------+-------------+\n",
            "only showing top 10 rows\n",
            "Request count by HTTP method:\n",
            "+------+------------+\n",
            "|method|method_count|\n",
            "+------+------------+\n",
            "|  POST|       25152|\n",
            "|DELETE|       24941|\n",
            "|   PUT|       24801|\n",
            "|   GET|       25106|\n",
            "+------+------------+\n",
            "\n",
            "+----------------------------+\n",
            "|Count of 404 responce codes:|\n",
            "+----------------------------+\n",
            "|                       25275|\n",
            "+----------------------------+\n",
            "\n",
            "Total responce size by day:\n",
            "+--------------------+-------------------+\n",
            "|                date|total_responce_size|\n",
            "+--------------------+-------------------+\n",
            "|2025-01-01 00:01:...|               9187|\n",
            "|2025-01-01 00:09:...|               4086|\n",
            "|2025-01-01 00:13:...|                384|\n",
            "|2025-01-01 00:13:...|               1980|\n",
            "|2025-01-01 00:21:...|               8654|\n",
            "|2025-01-01 00:28:...|               7405|\n",
            "|2025-01-01 00:35:...|               1837|\n",
            "|2025-01-01 00:40:...|               5017|\n",
            "|2025-01-01 00:49:...|               9911|\n",
            "|2025-01-01 00:50:...|               6363|\n",
            "|2025-01-01 00:51:...|               2471|\n",
            "|2025-01-01 00:52:...|               5154|\n",
            "|2025-01-01 00:57:...|               3533|\n",
            "|2025-01-01 01:00:...|               2838|\n",
            "|2025-01-01 01:00:...|               4904|\n",
            "|2025-01-01 01:11:...|               6268|\n",
            "|2025-01-01 01:16:...|               3654|\n",
            "|2025-01-01 01:16:...|               9192|\n",
            "|2025-01-01 01:18:...|               5359|\n",
            "|2025-01-01 01:28:...|               9157|\n",
            "+--------------------+-------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    }
  ]
}